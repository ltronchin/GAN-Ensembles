import sys
sys.path.extend([
    "./",
])
import os
import torch
import scipy
import numpy as np
import argparse
import hashlib
import  yaml
import ssl

ssl._create_default_https_context = ssl._create_unverified_context


from src.general_utils import util_data
from src.general_utils import util_general
import src.custom_metrics.preparation as pp
from src.custom_metrics import features

def get_parser():

    parser = argparse.ArgumentParser(description='Compute features.')

    parser.add_argument("-cfg", "--cfg_file", type=str, default="./src/configs_features/pneumoniamnist/feat_bin_real.yaml")
    parser.add_argument('--source_dir', type=str, default='./reports/pneumoniamnist', help='Directory name to fake samples.') #/home/lorenzo/GAN-Ensembles/reports/
    parser.add_argument('--dataset_name', type=str, default='pneumoniamnist', choices=['pneumoniamnist', 'retinamnist', 'breastmnist'],  help='The name of dataset')
    parser.add_argument('--gpu_ids', type=str, default='1', help='gpu ids: e.g. 0  use -1 for CPU')
    parser.add_argument('--batch_size', type=int, default='64', help='Batch size')
    parser.add_argument("--post_resizer", type=str, default="friendly", help="which resizer will you use to evaluate GANs in ['legacy', 'clean', 'friendly']")
    parser.add_argument('--eval_backbone', type=str, default='InceptionV3_torch', help="InceptionV3_torch, SwAV_torch, resnet_ae_50_pneumoniamnist, disc_resnet_50_pneumoniamnist, cnn_resnet50_pneumoniamnist, resnet_ae_50_retinamnist, disc_resnet_50_retinamnist, cnn_resnet50_retinamnist, resnet_ae_50_breastmnist, disc_resnet_50_breastmnist, cnn_resnet50_breastmnist ")
    parser.add_argument('--n_samples', type=int, default='4708',  help='Total number of images generated by the ensemble.')

    return parser

# main
if __name__ == '__main__':

    parser = get_parser()
    args, unknown = parser.parse_known_args()

    with open(args.cfg_file) as file:
        cfg = yaml.load(file, Loader=yaml.FullLoader)

    # Directories.
    source_dir = args.source_dir
    dataset_name = args.dataset_name
    samples_dir = os.path.join(source_dir, 'samples')
    cache_dir = os.path.join(source_dir, 'features')

    # Parameters.
    eval_backbone= args.eval_backbone
    post_resizer= args.post_resizer
    batch_size = args.batch_size
    max_items = args.n_samples
    # GANs
    gan_folder_name = 'pneumoniamnist-StyleGAN2-train-2023_10_02_17_17_54'

    filename = f'gradcam_{gan_folder_name}_{eval_backbone}_{post_resizer}'

    # Device.
    gpu_ids = int(args.gpu_ids)
    if gpu_ids >= 0:
        # Check if available.
        if not torch.cuda.is_available():
            device = torch.device('cpu')
        else:
            device = torch.device('cuda:{}'.format(gpu_ids))
    else:
        device = torch.device('cpu')

    # Cache features file.
    cache_dir_real = os.path.join(cache_dir, 'real_train')
    cache_dir_synth = os.path.join(cache_dir, gan_folder_name, 'fake', 'step=100000')
    args = dict(eval_backbone=eval_backbone, post_resizer=post_resizer, max_items=max_items, stats_kwargs={'capture_all': True, 'capture_mean_cov': True})
    md5 = hashlib.md5(repr(sorted(args.items())).encode('utf-8'))
    cache_tag = f"{max_items}-{eval_backbone}-{post_resizer}-{md5.hexdigest()}"
    cache_file_real = os.path.join(cache_dir_real, cache_tag + '.pkl')
    cache_file_synth = os.path.join(cache_dir_synth,  cache_tag + '.pkl')

    eval_model = pp.LoadEvalModel(
        eval_backbone=eval_backbone,
        post_resizer=post_resizer,
        device=device
    )

    # Dataset
    samples_dir_synth = os.path.join(samples_dir, gan_folder_name, 'fake', 'step=100000')
    dataset = util_data.EnsembleDataset(folders=[samples_dir_synth], weights=[1.0])
    num_batches = 1  # len(dataset) // batch_size

    # Dataloder
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)

    # Load fixed statistics.
    stats_real = features.FeatureStats.load(cache_file_real)
    stats_synt = features.FeatureStats.load(cache_file_synth)
    mu_real, sigma_real = stats_real.get_mean_cov()
    mu_synth, sigma_synth = stats_synt.get_mean_cov()

    eval_model.eval()
    data_iter = iter(dataloader)
    for _ in range(0, num_batches):
        try:
            images, labels = next(data_iter)
        except StopIteration:
            break

        images, labels = images.to(device), labels.to(device)
        images = torch.clamp(images, -1, 1) # Clip between -1 and 1

        features, _ = eval_model.get_outputs(images, quantize=True)

        # todo Spatial average

        # Update statistics.
        # todo N, mu_real, mu_synth, sigma_real, sigma_synth
        #sigma_synt_prime
        #mu_synth_prime
        # Compute updated mean and covariance, when reference image is added to set of gen. images.
        mean = ((num_images - 1) / num_images) * mean_gen + (1 / num_images) * features
        cov = ((num_images - 2) / (num_images - 1)) * cov_gen + \
              (1 / num_images) * torch.mm((features - mean_gen).T, (features - mean_gen))

        # Compute the fid.
        # todo transform to differentiable
        mean_term = torch.sum(torch.square(mean_reals - mean_gen.squeeze(0)))
        eigenvalues, _ = torch.eig(torch.matmul(cov_gen, cov_reals),
                                   eigenvectors=True)  # Eigenvalues shape: (D, 2) (real and imaginary parts).
        cov_term = torch.trace(cov_reals) + torch.trace(cov_gen) - 2 * torch.sum(torch.sqrt(eigenvalues[:, 0] + eps))
        wasserstein2 = mean_term + cov_term


        # Todo gradcam


    print("May the force be with you.")



