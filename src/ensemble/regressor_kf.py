import sys
sys.path.extend([
    "./",
])
import os
import torch
from itertools import combinations
from tqdm import tqdm
import math
import time
import scipy
import numpy as np
import pandas as pd
import uuid
import argparse

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import LeaveOneOut, KFold
from sklearn.metrics import mean_squared_error

import pickle

def get_parser():

    parser = argparse.ArgumentParser(description='Compute features.')

    parser.add_argument('--reports_dir', type=str, default='./reports/pneumoniamnist/ensemble/', help='Directory name to fake samples.') #/home/lorenzo/GAN-Ensembles/reports/pneumoniamnist #./reports/pneumoniamnist
    parser.add_argument('--dataset_name', type=str, default='pneumoniamnist', choices=['pneumoniamnist'],    help='The name of dataset')
    parser.add_argument("--post_resizer", type=str, default="friendly", help="which resizer will you use to evaluate GANs in ['legacy', 'clean', 'friendly']")
    parser.add_argument('--eval_backbone', type=str, default='cnn_resnet_50_pneumoniamnist', help="[InceptionV3_torch, ResNet50_torch, SwAV_torch, resnet_ae_50_pneumoniamnist, disc_resnet_50_pneumoniamnist, cnn_resnet_50_pneumoniamnist")
    parser.add_argument('--n_samples', type=int, default='50000',  help='Total number of images generated by the ensemble.')
    parser.add_argument('--split', type=str, default='train', choices=['train', 'val'])
    return parser

# main
if __name__ == '__main__':

    parser = get_parser()
    args, unknown = parser.parse_known_args()

    # Directories.
    reports_dir = args.reports_dir

    # Parameters.
    dataset_name = args.dataset_name
    eval_backbone = args.eval_backbone
    post_resizer = args.post_resizer
    n_samples = args.n_samples
    split = args.split
    model_name = 'dt' # dt, rf, lr

    # Load the dataset.
    filename_prdc = f'history_prdc_bin_real_{split}_{eval_backbone}_{post_resizer}_{n_samples}'
    filename_fid = f'history_fid_bin_real_{split}_{eval_backbone}_{post_resizer}_{n_samples}'

    # Load excel file.
    df_prdc = pd.read_excel(os.path.join(reports_dir, f'prdc/{filename_prdc}.xlsx'), engine='openpyxl')
    df_fid = pd.read_excel(os.path.join(reports_dir, f'fid/{filename_fid}.xlsx'), engine='openpyxl')
    df_rep = pd.read_excel(os.path.join(reports_dir, f'{dataset_name}_overall_reports.xlsx'), engine='openpyxl')

    gan_list_prdc = list(df_prdc['gan0'])
    step_list_prdc = list(df_prdc['step0'])
    gan_list_fid = list(df_prdc['gan0'])
    step_list_fid = list(df_prdc['step0'])

    assert gan_list_prdc == gan_list_fid
    assert step_list_prdc == step_list_fid

    gan_list = gan_list_prdc
    step_list = step_list_prdc

    df_rep_ord = pd.DataFrame(columns=['gan', 'step', 'ACC'])
    for gan, step in zip(gan_list, step_list):
        print(gan)
        print(step)
        row = df_rep[(df_rep['gan'] == gan) & (df_rep['step'] == str(step))]

        for index, row in row.iterrows():
            df_rep_ord = df_rep_ord.append({'gan': gan, 'step': step, 'ACC': row['ACC']}, ignore_index=True)

    df = pd.concat([df_prdc[['prc', 'rec', 'dns', 'cvg']],  df_fid[['fid']], df_rep_ord['ACC']], axis=1)

    X = df[['prc', 'rec', 'dns', 'cvg', 'fid']]
    y = df[['ACC']]

    # Use LeaveOneOut cross-validation
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    r2_scores = []
    feature_importances = []

    for train_index, test_index in tqdm(kf.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Standardize
        scaler = StandardScaler()
        scaler.fit(X_train)
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)

        # Train the model.
        if model_name == 'lr':
            reg = LinearRegression().fit(X_train, y_train.values.ravel())
        elif model_name == 'dt':
            reg = DecisionTreeRegressor(random_state=42).fit(X_train, y_train.values.ravel())
        elif model_name == 'rf':
            reg = RandomForestRegressor(random_state=42).fit(X_train, y_train.values.ravel())
        else:
            raise ValueError(model_name)

        # Predict the test set.
        y_pred = reg.predict(X_test)

        # Compute the R2 score for the current fold.
        r2_scores.append(r2_score(y_test.values.ravel(), y_pred))

        # Collect feature importances from the model.
        if model_name == 'lr':
            feature_importances.append(reg.coef_)
        elif model_name in ['dt', 'rf']:
            feature_importances.append(reg.feature_importances_)
        else:
            raise ValueError(model_name)

    # Calculate average R2 score and average feature importances.
    avg_r2 = np.mean(r2_scores)
    avg_feature_importances = np.mean(feature_importances, axis=0)

    # Calculate standard error of the mean for R2 scores and feature importances.
    sem_r2 = scipy.stats.sem(r2_scores)
    sem_feature_importances = scipy.stats.sem(feature_importances, axis=0)

    print(f'Average R2 score: {avg_r2} ± {sem_r2}')
    print(f'Average feature importances: {avg_feature_importances} ± {sem_feature_importances}')

    # Save to text file the values and save it.
    with open(os.path.join(reports_dir,  f'regressor_{model_name}_{dataset_name}_{eval_backbone}_{post_resizer}_{n_samples}.txt'),  'w') as f:
        f.write(f"Average R2 score: {avg_r2} ± {sem_r2}\n")
        f.write(f"features: {list(X.keys())}\n")
        f.write(f"Average feature importances: {avg_feature_importances} ± {sem_feature_importances}\n")


    print("May the force be with you.")
